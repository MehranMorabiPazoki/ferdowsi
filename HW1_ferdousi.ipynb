{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDahmgQGem9p"
      },
      "source": [
        "# Prepair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbIUd6pCFuCk",
        "outputId": "e67151ee-1471-46e7-b99f-bd5f8f30fe50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warning copy ferdousi.txt in this directory or copy form google drive form correct path!\n"
      ],
      "metadata": {
        "id": "pdWYuIDzz2TS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SjIOMrT9FuCn"
      },
      "outputs": [],
      "source": [
        "# !cp /content/drive/MyDrive/HW4_Q1/ferdousi.txt .\n",
        "!cp /content/drive/MyDrive/HW4_Q1_notebook/ferdousi.txt ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BP-5Jrudn73Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361b48e4-b473-4174-9e0d-6c11cadedb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (1.21.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.8.10)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.9.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.0.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install sacrebleu\n",
        "!pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "0R-_9DG8tBBw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xAAMKtbEFuCo"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ObbPHJJvd57y"
      },
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqCLrrSHmaTj",
        "outputId": "1368382c-8bed-44b1-e182-ceac459ced55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/Text-Mining/Persian-Wikipedia-Corpus/raw/master/models/glove/vectors.zip\n",
            "To: /content/vectors.zip\n",
            "100%|██████████| 48.2M/48.2M [00:00<00:00, 203MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  vectors.zip\n",
            "  inflating: vectors.txt             \n"
          ]
        }
      ],
      "source": [
        "gdown.download(url=\"https://github.com/Text-Mining/Persian-Wikipedia-Corpus/raw/master/models/glove/vectors.zip\",output=\"vectors.zip\")\n",
        "!unzip \"vectors.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXUYdGpMFuCq"
      },
      "source": [
        "# Data PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "R9CGMKbXFuCs"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "MAX_LENGTH = 0\n",
        "\n",
        "#initialize Lang Class\n",
        "class Lang:\n",
        "   def __init__(self):\n",
        "       #initialize containers to hold the words and corresponding index\n",
        "       self.word2index = {}\n",
        "       self.word2count = {}\n",
        "       self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"} #\n",
        "       self.n_words = 3  # Count SOS and EOS and PAD\n",
        "\n",
        "#split a sentence into words and add it to the container\n",
        "   def addSentence(self, sentence):\n",
        "       for word in sentence.split(' '):\n",
        "           self.addWord(word)\n",
        "\n",
        "#If the word is not in the container, the word will be added to it, \n",
        "#else, update the word counter\n",
        "   def addWord(self, word):\n",
        "       if word not in self.word2index:\n",
        "           self.word2index[word] = self.n_words\n",
        "           self.word2count[word] = 1\n",
        "           self.index2word[self.n_words] = word\n",
        "           self.n_words += 1\n",
        "       else:\n",
        "           self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S6F9ZwDiFuCt"
      },
      "outputs": [],
      "source": [
        "def read_sentence(df, lang1, lang2):\n",
        "   sentence1 = df[lang1]\n",
        "   sentence2 = df[lang2]\n",
        "   return sentence1, sentence2\n",
        "   \n",
        "def Word2vec(path=\"/content/vectors.txt\"):\n",
        "  file=open(path, mode=\"r\")\n",
        "  L=file.readlines(-1)\n",
        "  Word_feature={}\n",
        "  for i in L:\n",
        "    vect=i.split()\n",
        "    Word_feature[vect[0]]=np.array(vect[1:],dtype=float)\n",
        "  return Word_feature\n",
        "\n",
        "def read_file(loc, lang1, lang2):\n",
        "   df = pd.read_csv(loc, delimiter='\\n', header=None, names=['Beyt'])[2::]\n",
        "   d = {lang1:df[0::2].values[:,0].tolist() , lang2: df[1::2].values[:,0].tolist()}\n",
        "   df=pd.DataFrame(d)\n",
        "   return df\n",
        "\n",
        "def process_data(lang1,lang2):\n",
        "   global MAX_LENGTH\n",
        "   df = read_file('ferdousi.txt', lang1, lang2)\n",
        "   print(\"Read %s sentence pairs\" % len(df))\n",
        "   sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
        "  #  print(sentence1[0])\n",
        "   source = Lang()\n",
        "   target = Lang()\n",
        "   pairs = []\n",
        "\n",
        "   for i in range(len(df)):\n",
        "      if len(sentence1[i].split(' '))>MAX_LENGTH:\n",
        "         MAX_LENGTH = len(sentence1[i].split(' '))\n",
        "     \n",
        "      full = [sentence1[i], sentence2[i]]\n",
        "      source.addSentence(sentence1[i])\n",
        "      target.addSentence(sentence2[i])\n",
        "      pairs.append(full)\n",
        "\n",
        "   print(f\"Maximum lenght of input is {MAX_LENGTH}\")\n",
        "\n",
        "   return source, target, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_mlMEoMFuCu",
        "outputId": "cf841d20-26e6-4b56-946e-ab29625b92d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 49609 sentence pairs\n",
            "Maximum lenght of input is 11\n"
          ]
        }
      ],
      "source": [
        "lang1 = 'M1'\n",
        "lang2 = 'M2'\n",
        "# a=read_file('ferdousi.txt', lang1, lang2)\n",
        "source, target, pairs = process_data(lang1, lang2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "I6XufbzrFuCw"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "   return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "   global PAD_token\n",
        "   global MAX_LENGTH\n",
        "   indexes = indexesFromSentence(lang, sentence)\n",
        "   pad_list=[PAD_token for i in range(MAX_LENGTH-len(indexes))]\n",
        "   indexes =  indexes  + pad_list\n",
        "   indexes.append(EOS_token)\n",
        "   return torch.tensor(indexes, dtype=torch.long,device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "   input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "   target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "   return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1Yw3WLD_FuCx"
      },
      "outputs": [],
      "source": [
        "training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                     for i in range(10)]\n",
        "                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rk7L_RqRJJ8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44109603-0667-43e8-fc44-c486062de82d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([[ 106],\n",
              "          [1269],\n",
              "          [  48],\n",
              "          [ 176],\n",
              "          [ 705],\n",
              "          [  37],\n",
              "          [1161],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[ 110],\n",
              "          [  83],\n",
              "          [1429],\n",
              "          [  13],\n",
              "          [  83],\n",
              "          [4805],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[1420],\n",
              "          [   7],\n",
              "          [3896],\n",
              "          [   7],\n",
              "          [2071],\n",
              "          [ 490],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[  52],\n",
              "          [4177],\n",
              "          [ 184],\n",
              "          [ 183],\n",
              "          [ 208],\n",
              "          [ 705],\n",
              "          [ 705],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[ 328],\n",
              "          [  83],\n",
              "          [5853],\n",
              "          [ 842],\n",
              "          [ 464],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[ 178],\n",
              "          [ 117],\n",
              "          [  25],\n",
              "          [  28],\n",
              "          [ 201],\n",
              "          [ 159],\n",
              "          [6596],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[ 135],\n",
              "          [  19],\n",
              "          [  38],\n",
              "          [ 352],\n",
              "          [2773],\n",
              "          [ 213],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[ 296],\n",
              "          [3176],\n",
              "          [2465],\n",
              "          [  58],\n",
              "          [ 156],\n",
              "          [3366],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[3238],\n",
              "          [ 371],\n",
              "          [ 235],\n",
              "          [ 473],\n",
              "          [ 334],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[1383],\n",
              "          [  69],\n",
              "          [2605],\n",
              "          [  24],\n",
              "          [ 141],\n",
              "          [  52],\n",
              "          [  32],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[117],\n",
              "          [ 19],\n",
              "          [ 38],\n",
              "          [620],\n",
              "          [ 68],\n",
              "          [ 72],\n",
              "          [  2],\n",
              "          [  2],\n",
              "          [  2],\n",
              "          [  2],\n",
              "          [  2],\n",
              "          [  1]], device='cuda:0'), tensor([[  67],\n",
              "          [ 465],\n",
              "          [1290],\n",
              "          [  13],\n",
              "          [ 630],\n",
              "          [  27],\n",
              "          [ 105],\n",
              "          [ 456],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[1155],\n",
              "          [ 157],\n",
              "          [   7],\n",
              "          [2046],\n",
              "          [2504],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[  296],\n",
              "          [   27],\n",
              "          [  291],\n",
              "          [  567],\n",
              "          [11170],\n",
              "          [ 1541],\n",
              "          [    2],\n",
              "          [    2],\n",
              "          [    2],\n",
              "          [    2],\n",
              "          [    2],\n",
              "          [    1]], device='cuda:0')), (tensor([[  21],\n",
              "          [  83],\n",
              "          [3410],\n",
              "          [1130],\n",
              "          [ 397],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[1237],\n",
              "          [  24],\n",
              "          [  71],\n",
              "          [ 783],\n",
              "          [2034],\n",
              "          [ 250],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[  21],\n",
              "          [  83],\n",
              "          [1130],\n",
              "          [  80],\n",
              "          [ 129],\n",
              "          [1473],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[  25],\n",
              "          [  43],\n",
              "          [ 653],\n",
              "          [ 385],\n",
              "          [4156],\n",
              "          [1886],\n",
              "          [1609],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0')), (tensor([[  33],\n",
              "          [1008],\n",
              "          [ 188],\n",
              "          [  37],\n",
              "          [2605],\n",
              "          [2226],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'), tensor([[2936],\n",
              "          [  13],\n",
              "          [ 959],\n",
              "          [ 112],\n",
              "          [ 448],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   2],\n",
              "          [   1]], device='cuda:0'))]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "training_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmnrWQT9FuCy"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bCQ5JSxqFuCy"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers,GRU=False):\n",
        "       super(Encoder, self).__init__()\n",
        "      \n",
        "       #set the encoder input dimesion , embbed dimesion, hidden dimesion, and number of layers \n",
        "       self.input_dim = input_dim\n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "       #initialize the embedding layer with input and embbed dimention\n",
        "       self.embedding = nn.Embedding(input_dim, self.embbed_dim,padding_idx=2) #\n",
        "       #intialize the GRU to take the input dimetion of embbed, and output dimention of hidden and\n",
        "       #set the number of gru layers\n",
        "       if GRU:\n",
        "        self.recurrent = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "       else:\n",
        "        self.recurrent = nn.LSTM(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "              \n",
        "   def forward(self, src):\n",
        "       \n",
        "       embedded = self.embedding(src).view(1,1,-1)\n",
        "       outputs, hidden = self.recurrent(embedded)\n",
        "       return outputs, hidden\n",
        "   def initHidden(self):\n",
        "     return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers,GRU=False):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "#set the encoder output dimension, embed dimension, hidden dimension, and number of layers \n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.output_dim = output_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "# initialize every layer with the appropriate dimension. For the decoder layer, it will consist of an embedding, GRU, a Linear layer and a Log softmax activation function.\n",
        "       self.embedding = nn.Embedding(output_dim, self.embbed_dim ,padding_idx=2) # \n",
        "       \n",
        "       if GRU:\n",
        "        self.recurrent = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "       else :\n",
        "        self.recurrent=nn.LSTM(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "       self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "   def forward(self, input, hidden):\n",
        "\n",
        "# reshape the input to (1, batch_size)\n",
        "       input = input.view(1, -1)\n",
        "       embedded = F.relu(self.embedding(input))\n",
        "       output, hidden = self.recurrent(embedded, hidden)       \n",
        "       prediction = self.softmax(self.out(output[0]))\n",
        "      \n",
        "       return prediction, hidden\n",
        "   def initHidden(self):\n",
        "      return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH,mode='GRU'):\n",
        "       super().__init__()\n",
        "      \n",
        "#initialize the encoder and decoder\n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "       self.mode = mode\n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) #get the input length (number of words in sentence)\n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "      \n",
        "#initialize a variable to hold the predicted outputs\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "#encode every word in a sentence\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "#use the encoder’s hidden layer as the decoder hidden\n",
        "       \n",
        "       if self.mode == \"GRU\":\n",
        "         decoder_hidden = encoder_hidden.to(device)\n",
        "       elif self.mode == \"LSTM\":\n",
        "         decoder_hidden = (encoder_hidden[0].to(device),encoder_hidden[1].to(device))\n",
        "       \n",
        "#add a token before the first predicted word\n",
        "       decoder_input = torch.tensor([SOS_token], device=device)  # SOS\n",
        "\n",
        "#topk is used to get the top K value over a list\n",
        "#predict the output word from the current target word. If we enable the teaching force,  then the #next decoder input is the next word, else, use the decoder output highest value. \n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == EOS_token):\n",
        "               break\n",
        "\n",
        "       return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Oj4Yc1UFuC-"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OJWRGTEDFuC-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "0c64a6a50d9e4c41a5f1d7591700c859",
            "1916a5d353a94dea80b79af06f33ce68",
            "df3788559ea5425396b38dfa6e55c1e5",
            "2390946e0905462894fb8c1959e42fe8",
            "672277e98bee42e6b9c521307bb0060e",
            "30ebee065ee64ce68dfacd1eb122f997",
            "314094a449444440bd65e2472b7425b3",
            "06471efb792b4cf0b4605edc272142b4",
            "0c55ffad6a8246aa97c10d4a4e9f0c9d",
            "d8472e955bf74dc1a43e16ec64d006f3",
            "bad9a9f9fc6f466b97c0bca94343ae77"
          ]
        },
        "outputId": "2b661dfc-2e48-42f3-e9b1-a6ca8f41de2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-4d2c4558e126>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric('sacrebleu')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c64a6a50d9e4c41a5f1d7591700c859"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "import datasets\n",
        "\n",
        "metric = datasets.load_metric('sacrebleu')\n",
        "\n",
        "def clacModel(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "\n",
        "   model_optimizer.zero_grad()\n",
        "\n",
        "   input_length = input_tensor.size(0)\n",
        "   loss = 0\n",
        "   epoch_loss = 0\n",
        "   # print(input_tensor.shape)\n",
        "\n",
        "   output = model(input_tensor, target_tensor,teacher_forcing_ratio)\n",
        "\n",
        "   num_iter = output.size(0)\n",
        "  #  print(num_iter)\n",
        "\n",
        "#calculate the loss from a predicted sentence with the expected result\n",
        "   for ot in range(num_iter):\n",
        "       loss += criterion(output[ot], target_tensor[ot])\n",
        "       \n",
        "   metric.add(predictions=output, references=target_tensor)\n",
        "   \n",
        "   loss.backward()\n",
        "   model_optimizer.step()\n",
        "   epoch_loss = loss.item() / num_iter\n",
        "\n",
        "   return epoch_loss,metric\n",
        "\n",
        "def trainModel(model, source, target, pairs, num_iteration=20000,type='GRU'):\n",
        "   \n",
        "   model.train()\n",
        "\n",
        "  #  optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "   optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-5)\n",
        "  #  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=num_iteration//5000,eta_min=1e-4,last_epoch=-1)\n",
        "  #  criterion = nn.NLLLoss()\n",
        "   criterion = nn.CrossEntropyLoss()\n",
        "   total_loss_iterations = 0\n",
        "   final_score = 0\n",
        "   training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                     for i in range(num_iteration)]\n",
        "  \n",
        "   for iter in tqdm(range(1, num_iteration+1)):\n",
        "       training_pair = training_pairs[iter - 1]\n",
        "       input_tensor = training_pair[0]\n",
        "       target_tensor = training_pair[1]\n",
        "\n",
        "       loss,metric = clacModel(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "       s  = metric.compute()\n",
        "\n",
        "       final_score += s['score']\n",
        "       \n",
        "       total_loss_iterations += loss\n",
        "       writer.add_scalar('Loss', loss,iter)\n",
        "       writer.add_scalar('score', s['score'],iter) \n",
        "\n",
        "       if iter % 5000 == 0:\n",
        "           avarage_loss= total_loss_iterations / 5000\n",
        "           avg_score = final_score /5000\n",
        "           total_loss_iterations = 0\n",
        "           tqdm.write(f'iter :{iter} , Loss : {avarage_loss} , score :{avg_score}')\n",
        "          #  scheduler.step()\n",
        "           torch.save(model.state_dict(), f'/content/drive/MyDrive/HW4_Q1/{type}_Model.pt')\n",
        "          \n",
        "   return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6otv8sFuC_"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Gabw_5sDFuC_"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
        "        output_tensor = tensorFromSentence(output_lang, sentences[1])\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        output = model(input_tensor, output_tensor)\n",
        "        # print(output_tensor)\n",
        "\n",
        "        for ot in range(output.size(0)):\n",
        "            topv, topi = output[ot].topk(1)\n",
        "            # print(topi)\n",
        "\n",
        "            if topi[0].item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, source, target, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('source {}'.format(pair[0]))\n",
        "        print('target {}'.format(pair[1]))\n",
        "        output_words = evaluate(model, source, target, pair)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('predicted {}'.format(output_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wd1JctGVFuDA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xky23N_RFuDA"
      },
      "source": [
        "# train GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNrsWxs_FuDA",
        "outputId": "83ae6a07-0003-4602-a6fb-63e66e71b09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 49609 sentence pairs\n",
            "Maximum lenght of input is 11\n",
            "random sentence ['چو بخشنده ام بیش بسپارمش', 'کلاه از بر چرخ بگذارمش']\n",
            "Input : 12687 Output : 13323\n",
            "Encoder(\n",
            "  (embedding): Embedding(12687, 256, padding_idx=2)\n",
            "  (recurrent): GRU(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(13323, 256, padding_idx=2)\n",
            "  (recurrent): GRU(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=13323, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 4999/100000 [02:54<54:59, 28.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :5000 , Loss : 3.6835600317001482 , score :0.30085014668087895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 10000/100000 [05:48<1:20:48, 18.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :10000 , Loss : 3.498808099969217 , score :0.5929966326562244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 15000/100000 [08:50<1:15:49, 18.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :15000 , Loss : 3.456614892419176 , score :0.8847852617226406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 19998/100000 [11:52<47:10, 28.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :20000 , Loss : 3.4310910407384325 , score :1.1698831124777924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 24999/100000 [14:49<43:55, 28.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :25000 , Loss : 3.4265187446912235 , score :1.4549786330733296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 29999/100000 [17:46<40:16, 28.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :30000 , Loss : 3.4146651433944624 , score :1.7400763999463467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 34998/100000 [20:52<40:09, 26.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :35000 , Loss : 3.396766833464302 , score :2.025135047119711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 39999/100000 [23:57<37:29, 26.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :40000 , Loss : 3.4081081892967244 , score :2.3101928686417703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 45000/100000 [27:02<49:27, 18.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :45000 , Loss : 3.4064408769925425 , score :2.595247639205983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 49998/100000 [30:05<30:48, 27.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :50000 , Loss : 3.3961033751487855 , score :2.8803054816986564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 54999/100000 [33:10<27:29, 27.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :55000 , Loss : 3.404873932902023 , score :3.1645525111728348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 60000/100000 [36:15<35:56, 18.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :60000 , Loss : 3.400897659842174 , score :3.4489375964895967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 64998/100000 [39:18<21:49, 26.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :65000 , Loss : 3.3876089165051746 , score :3.733239543205024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 69999/100000 [42:23<18:30, 27.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :70000 , Loss : 3.403670941670735 , score :4.0164240766089305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 75000/100000 [45:27<22:41, 18.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :75000 , Loss : 3.3896361064275085 , score :4.300357620164314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 79998/100000 [48:32<12:09, 27.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :80000 , Loss : 3.4077032384872408 , score :4.58256508431767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 84999/100000 [51:37<09:20, 26.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :85000 , Loss : 3.3993611530303918 , score :4.866305654789221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 90000/100000 [54:42<08:57, 18.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :90000 , Loss : 3.3982938988049916 , score :5.151107958591581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 94998/100000 [57:47<03:01, 27.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :95000 , Loss : 3.3996316342989608 , score :5.435722628486341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [1:00:52<00:00, 27.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :100000 , Loss : 3.3889591455459644 , score :5.720267126350617\n",
            "source بزد یک دم آن اژدهای پلید\n",
            "target تنی چند ازیشان به دم درکشید\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source ازان پس چو فرمایدم شهریار\n",
            "target بیایم پرستش کنم بنده وار\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source همی بوی مهر آمد از روی او\n",
            "target همی زیب تاج آمد از موی او\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source چنین گفت کز بارگاه بلند\n",
            "target برفتم سوی رستم دیوبند\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source بخوردند با شتاب چیزی که بود\n",
            "target پس آنگه به زمزم بگفتند زود\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source چو بشنید افراسیاب این سخن\n",
            "target پشیمان شد از کرده های کهن\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source چنین داد پاسخ که دانش بهست\n",
            "target چو دانا بود برمهان برمهست\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source گر ایدون که بر من نسازید بد\n",
            "target کنید آنک از داد و گردی سزد\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source بدو گفت شاه این کجا داشتی\n",
            "target مرا مست کردی و بگذاشتی\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source از افراز چون کژ گردد سپهر\n",
            "target نه تندی بکار آید از بن نه مهر\n",
            "predicted که از و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n"
          ]
        }
      ],
      "source": [
        "lang1 = 'M1'\n",
        "lang2 = 'M2'\n",
        "writer = SummaryWriter(log_dir=\"/content/drive/MyDrive/HW4_Q1/GRU_logs\")\n",
        "source, target, pairs = process_data(lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "#print number of words\n",
        "input_size = source.n_words\n",
        "output_size = target.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "num_iteration = 100000\n",
        "\n",
        "#create encoder-decoder model\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers,GRU=True)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers,GRU=True)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device,mode='GRU').to(device)\n",
        "\n",
        "#print model \n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "model = trainModel(model, source, target, pairs, num_iteration,type='GRU')\n",
        "evaluateRandomly(model, source, target, pairs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "pm2VIpKGyxTR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set correct path of GRU_logs in your system\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/HW4_Q1/GRU_logs\""
      ],
      "metadata": {
        "id": "AHk1Gw7Qf7KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 64860"
      ],
      "metadata": {
        "id": "h3Tcj3vKy3E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train LSTM"
      ],
      "metadata": {
        "id": "fRwRoy7Cf7eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang1 = 'M1'\n",
        "lang2 = 'M2'\n",
        "writer = SummaryWriter(log_dir=\"/content/drive/MyDrive/HW4_Q1/LSTM_logs\")\n",
        "source, target, pairs = process_data(lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "#print number of words\n",
        "input_size = source.n_words\n",
        "output_size = target.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "num_iteration = 100000\n",
        "\n",
        "#create encoder-decoder model\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers,GRU=False)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers,GRU=False)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device,mode='LSTM').to(device)\n",
        "\n",
        "#print model \n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "model = trainModel(model, source, target, pairs, num_iteration,type='LSTM')\n",
        "evaluateRandomly(model, source, target, pairs)"
      ],
      "metadata": {
        "id": "j2w9YXZsaZ_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80de3db0-beec-4c75-ce18-3030e2dca3f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 49609 sentence pairs\n",
            "Maximum lenght of input is 11\n",
            "random sentence ['بدین کار در پارس گرد آمدند', 'بسی زین نشان داستانها زدند']\n",
            "Input : 12687 Output : 13323\n",
            "Encoder(\n",
            "  (embedding): Embedding(12687, 256, padding_idx=2)\n",
            "  (recurrent): LSTM(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(13323, 256, padding_idx=2)\n",
            "  (recurrent): LSTM(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=13323, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 4998/100000 [03:11<1:00:26, 26.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :5000 , Loss : 3.5659987035433542 , score :0.2978983691102564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 9999/100000 [06:26<58:31, 25.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :10000 , Loss : 3.4073303403854442 , score :0.5829569743414826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 15000/100000 [09:41<1:22:35, 17.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :15000 , Loss : 3.4057777534167095 , score :0.8680148168332474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 19998/100000 [12:55<52:19, 25.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :20000 , Loss : 3.399622452259068 , score :1.1530734430350886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 24999/100000 [16:09<47:27, 26.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :25000 , Loss : 3.398562683931982 , score :1.4381190607248526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 30000/100000 [19:22<1:07:14, 17.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :30000 , Loss : 3.377958038584395 , score :1.7229839091624468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▍      | 34998/100000 [22:35<41:31, 26.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :35000 , Loss : 3.383033891232812 , score :2.007980711527563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 39999/100000 [25:48<38:08, 26.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :40000 , Loss : 3.387989025402062 , score :2.2928005583375444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 45000/100000 [29:01<53:44, 17.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :45000 , Loss : 3.3956036910692906 , score :2.5776654067757714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|████▉     | 49998/100000 [32:14<32:17, 25.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :50000 , Loss : 3.3885641391754167 , score :2.862634750520263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 54999/100000 [35:27<28:31, 26.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :55000 , Loss : 3.3790883762995385 , score :3.1473180669665846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 60000/100000 [38:40<38:25, 17.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :60000 , Loss : 3.377978123188024 , score :3.432171474312885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▍   | 64998/100000 [41:53<22:02, 26.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :65000 , Loss : 3.3819395123481817 , score :3.717229295834944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 69999/100000 [45:06<19:05, 26.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :70000 , Loss : 3.3859584886232974 , score :4.002287117357003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 75000/100000 [48:19<23:35, 17.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :75000 , Loss : 3.3835992897033806 , score :4.287339599702831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 79998/100000 [51:32<12:49, 25.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :80000 , Loss : 3.382188943926482 , score :4.572398204934966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 84999/100000 [54:40<10:14, 24.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :85000 , Loss : 3.38394076156616 , score :4.857456026457025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 90000/100000 [57:43<09:22, 17.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :90000 , Loss : 3.3865597101847356 , score :5.142513847979084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 94998/100000 [1:00:45<03:03, 27.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :95000 , Loss : 3.387362746270491 , score :5.427571669501144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [1:03:48<00:00, 26.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter :100000 , Loss : 3.373803162066143 , score :5.712629491023203\n",
            "source گر آزار بودیش در دل ز من\n",
            "target سرم برنیفراختی ز انجمن\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source ز خویشان گزین کرد پیران هزار\n",
            "target پذیره شدن را برآراست کار\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source بیایید یکسر به درگاه من\n",
            "target که بر مرز بگذشت بد خواه من\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source تو بندیش هشیار و بگشای گوش\n",
            "target سخن از خردمند مردم نیوش\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source به تنها تن خویش جویم نبرد\n",
            "target ز لشکر نخواهم کسی رنجه کرد\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source نخست آفرین کرد بر کردگار\n",
            "target جهاندار پیروز و پروردگار\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source چنین داد پاسخ که از بیش خورد\n",
            "target مگر آرزو بازگردد بدرد\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source بدو گفت زال ای پسر این سخن\n",
            "target مگوی و جدا کن سرش را ز بن\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source چو هنگام رفتن فراز آیدت\n",
            "target به دیدار خسرو نیاز آیدت\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n",
            "source یکی برگزیند که نامی ترست\n",
            "target به خاقان چین برگرامی ترست\n",
            "predicted که و و و PAD PAD PAD PAD PAD PAD PAD <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set correct path of GRU_logs in your system\n",
        "%tensorboard --logdir \"/content/drive/MyDrive/HW4_Q1/LSTM_logs\""
      ],
      "metadata": {
        "id": "eiVI5TdorzSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wDahmgQGem9p",
        "FXUYdGpMFuCq",
        "BmnrWQT9FuCy",
        "zm6otv8sFuC_"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c64a6a50d9e4c41a5f1d7591700c859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1916a5d353a94dea80b79af06f33ce68",
              "IPY_MODEL_df3788559ea5425396b38dfa6e55c1e5",
              "IPY_MODEL_2390946e0905462894fb8c1959e42fe8"
            ],
            "layout": "IPY_MODEL_672277e98bee42e6b9c521307bb0060e"
          }
        },
        "1916a5d353a94dea80b79af06f33ce68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30ebee065ee64ce68dfacd1eb122f997",
            "placeholder": "​",
            "style": "IPY_MODEL_314094a449444440bd65e2472b7425b3",
            "value": "Downloading builder script: "
          }
        },
        "df3788559ea5425396b38dfa6e55c1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06471efb792b4cf0b4605edc272142b4",
            "max": 2848,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c55ffad6a8246aa97c10d4a4e9f0c9d",
            "value": 2848
          }
        },
        "2390946e0905462894fb8c1959e42fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8472e955bf74dc1a43e16ec64d006f3",
            "placeholder": "​",
            "style": "IPY_MODEL_bad9a9f9fc6f466b97c0bca94343ae77",
            "value": " 7.65k/? [00:00&lt;00:00, 392kB/s]"
          }
        },
        "672277e98bee42e6b9c521307bb0060e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ebee065ee64ce68dfacd1eb122f997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314094a449444440bd65e2472b7425b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06471efb792b4cf0b4605edc272142b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c55ffad6a8246aa97c10d4a4e9f0c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8472e955bf74dc1a43e16ec64d006f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad9a9f9fc6f466b97c0bca94343ae77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}